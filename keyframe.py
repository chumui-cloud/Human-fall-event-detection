# -*- coding: utf-8 -*-
"""KeyFrame.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1plNxPHhZblQMkcUTWUPaS8P5IAEyAZIr
"""


import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from mahotas.features import haralick,lbp
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
import math
import os
from threading import Thread

def RGB_histogram_thread(image, result, indx):
    bins = 8
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])
    cv2.normalize(hist, hist)
    #print(hist.shape)
    result[indx] = hist.flatten()

def feature_hog_thread(image, result, indx):
    img = image[:, :, ::-1]
    fd, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16),
                    cells_per_block=(1, 1), visualize=True, multichannel=True,feature_vector=True)
    result[indx] = fd

def lbp_feature_thread(image, result, indx):
    radius = 2
    n_points = 12
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    Features = lbp(gray, radius, n_points)
    result[indx] = Features

def haralick_texture_thread(image, result, indx):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    feature = haralick(gray).mean(axis=0)
    result[indx] = feature

def get_keyframe_list_thread(video_path,frame_count):
  features = []
  cap = cv2.VideoCapture(video_path)
  success, frame = cap.read()
  n_frame = int(cap. get(cv2. CAP_PROP_FRAME_COUNT))-1
  for i in tqdm(range(n_frame)):
    feature = [[] for i in range(4)]
    success, frame = cap.read()
    frame = cv2.resize(frame,(224,224))
    f_hist = Thread(target=RGB_histogram_thread, args=(frame, feature, 0))
    f_hog = Thread(target=feature_hog_thread, args=(frame, feature, 1))
    #f_lbp = Thread(target=lbp_feature_thread, args=(frame, feature, 2))
    #f_tex = Thread(target=haralick_texture_thread, args=(frame, feature, 3))
    threads = [f_hist,f_hog]#,f_lbp,f_tex]
    for thread in threads:
      thread.start()
    for thread in threads:
      thread.join()
    f = np.hstack(
          [feature[0],feature[1],feature[2],feature[3]
          ]
      )
    features.append(f)
  kmn = KMeans(frame_count)
  kmn.fit(features)
  kmn = KMeans(frame_count).fit(features)
  closest, _ = pairwise_distances_argmin_min(kmn.cluster_centers_, features)
  closest.sort()
  return closest, kmn.labels_

def RGB_histogram(image, mask=None):
    bins = 8
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])
    cv2.normalize(hist, hist)
    #print(hist.shape)
    return hist.flatten()

def feature_hog(image):
    img = image[:, :, ::-1]
    fd, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16),
                    cells_per_block=(1, 1), visualize=True, multichannel=True,feature_vector=True)
    return fd

def lbp_feature(image):
    radius = 2
    n_points = 12
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    Features = lbp(gray, radius, n_points)
    return Features

def haralick_texture(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    feature = haralick(gray).mean(axis=0)
    return feature
  
def count(arr):
    mp = dict()
    count_dis=0
    for i in range(len(arr)):
        if arr[i] in mp.keys():
            mp[arr[i]] += 1
        else:
            mp[arr[i]] = 1

    return mp

def get_keyframe_list(video_path,frame_count):
  features = []
  cap = cv2.VideoCapture(video_path)
  success, frame = cap.read()
  n_frame = int(cap. get(cv2. CAP_PROP_FRAME_COUNT))-1
  for i in tqdm(range(n_frame)):
    success, frame = cap.read()
    frame = cv2.resize(frame,(224,224))
    f_hist = RGB_histogram(frame)
    f_hog = feature_hog(frame)
    f_lbp = lbp_feature(frame)
    f_tex = haralick_texture(frame)
    feature = np.hstack(
          [f_hist,f_hog,f_lbp,f_tex
          ]
      )
    features.append(feature)
  kmn = KMeans(frame_count)
  kmn.fit(features)
  kmn = KMeans(frame_count).fit(features)
  closest, _ = pairwise_distances_argmin_min(kmn.cluster_centers_, features)
  closest.sort()
  return closest, kmn.labels_

def plot_keyframes(video_path,keyframes):
  cap = cv2.VideoCapture(video_path)
  i = 0
  start = 3
  for x in range(start,1000):
    if len(keyframes)%x==0:
      y = int(len(keyframes)/x)
      break
  f, ax = plt.subplots(y,x,figsize=(x*3,y*3))
  for ax1 in ax:
    for ax2 in ax1:
      cap.set(1,keyframes[i]);
      success, frame = cap.read()
      i = i+1
      ax2.imshow(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))
      ax2.axis('off')
  f.tight_layout()

def print_labels(labels):
  l = len(labels)+1
  ll = int(math.sqrt(l))
  for i in range(1,l):
    print(labels[i-1],end="  ")
    if i % ll == 0:
      print()
  print()

def save_keyframes(video_path,save_path,keyframes):
  cap = cv2.VideoCapture(video_path)
  if not os.path.isdir(save_path):
    os.mkdir(save_path)
  for i in keyframes:
    cap.set(1,i)
    _, frame = cap.read()
    if frame is None:
      continue
    print(save_path+'/Frame'+str(i)+'.jpeg')
    cv2.imwrite(save_path+'/Frame'+str(i)+'.jpeg',cv2.cvtColor(frame,cv2.COLOR_BGR2RGB),[cv2.IMWRITE_JPEG_QUALITY, 100])
