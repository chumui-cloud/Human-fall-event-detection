# -*- coding: utf-8 -*-
"""ProcessWithMRCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12tiWChzXErOrdZux3_ub1yyBeQ_fsKb1
"""

#!git clone https://github.com/tensorflow/tpu/

# Commented out IPython magic to ensure Python compatibility.
from IPython import display
from PIL import Image
import numpy as np
# %tensorflow_version 1.x
import tensorflow as tf
import sys
import base64
sys.path.insert(0, 'tpu/models/official')
sys.path.insert(0, 'tpu/models/official/mask_rcnn')
import coco_metric
from mask_rcnn.object_detection import visualization_utils
import cv2

try:
  import os
  import pprint

  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print('TPU address is', TPU_ADDRESS)

  session = tf.compat.v1.Session(TPU_ADDRESS, graph=tf.Graph())
  print('TPU devices:')
  pprint.pprint(session.list_devices())
except:
  session = tf.compat.v1.Session(graph=tf.Graph())

saved_model_dir = 'gs://cloud-tpu-checkpoints/mask-rcnn/1555659850' 
_ = tf.compat.v1.saved_model.loader.load(session, ['serve'], saved_model_dir)

def extract_person(image, extract_all = False):
  width = image.shape[0]
  height = image.shape[1]
  image_string = np.array([cv2.imencode('.jpg', image)[1].tobytes()])
  num_detections, detection_boxes, detection_classes, detection_scores, detection_masks, image_info = session.run(
    ['NumDetections:0', 'DetectionBoxes:0', 'DetectionClasses:0', 'DetectionScores:0', 'DetectionMasks:0', 'ImageInfo:0'],
    feed_dict={'Placeholder:0': image_string})

  num_detections = np.squeeze(num_detections.astype(np.int32), axis=(0,))
  detection_boxes = np.squeeze(detection_boxes * image_info[0, 2], axis=(0,))[0:num_detections]
  detection_scores = np.squeeze(detection_scores, axis=(0,))[0:num_detections]
  detection_classes = np.squeeze(detection_classes.astype(np.int32), axis=(0,))[0:num_detections]
  instance_masks = np.squeeze(detection_masks, axis=(0,))[0:num_detections]
  
  choices = np.where(detection_classes==1)[0]
  if choices.shape[0]==0:
    return np.zeros((height,width,3), np.uint8)
    
  if extract_all:
    index = np.where(detection_classes==1)[0]
    mask = np.array(instance_masks[index])
    cls = np.array(detection_classes[index])
    score = np.array(detection_scores[index])
    box = np.array(detection_boxes[index])
    ymin, xmin, ymax, xmax = np.split(box, 4, axis=-1)
  else:
    index = np.where(detection_classes==1)[0][0]
    mask = np.array([instance_masks[index]])
    cls = np.array([detection_classes[index]])
    score = np.array([detection_scores[index]])
    box = np.array([detection_boxes[index]])
    ymin, xmin, ymax, xmax = np.split(box, 4, axis=-1)
  processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)
  segmentations = coco_metric.generate_segmentation_from_masks(mask, processed_boxes, width, height)
  total_mask = segmentations[0]
  for mask in segmentations:
      total_mask = cv2.add(total_mask, mask)
  total_mask = total_mask.clip(0, 255).astype("uint8")
  output = cv2.bitwise_and(image,image,mask = total_mask)
  return output

#image = cv2.imread('rgb_0059.png')

#masked_image = extract_person(image)
#cv2.imwrite('masked1.jpg',masked_image)


def extract_person_mcfd(image, extract_all = False):
  width = image.shape[0]
  height = image.shape[1]
  image_string = np.array([cv2.imencode('.jpg', image)[1].tobytes()])
  num_detections, detection_boxes, detection_classes, detection_scores, detection_masks, image_info = session.run(
    ['NumDetections:0', 'DetectionBoxes:0', 'DetectionClasses:0', 'DetectionScores:0', 'DetectionMasks:0', 'ImageInfo:0'],
    feed_dict={'Placeholder:0': image_string})

  num_detections = np.squeeze(num_detections.astype(np.int32), axis=(0,))
  detection_boxes = np.squeeze(detection_boxes * image_info[0, 2], axis=(0,))[0:num_detections]
  detection_scores = np.squeeze(detection_scores, axis=(0,))[0:num_detections]
  detection_classes = np.squeeze(detection_classes.astype(np.int32), axis=(0,))[0:num_detections]
  instance_masks = np.squeeze(detection_masks, axis=(0,))[0:num_detections]
  
  choices = np.where(detection_classes==1)[0]
  if choices.shape[0]==0:
    return np.zeros((height,width,3), np.uint8), False
    
  if extract_all:
    index = np.where(detection_classes==1)[0]
    mask = np.array(instance_masks[index])
    cls = np.array(detection_classes[index])
    score = np.array(detection_scores[index])
    box = np.array(detection_boxes[index])
    ymin, xmin, ymax, xmax = np.split(box, 4, axis=-1)
  else:
    index = np.where(detection_classes==1)[0][0]
    mask = np.array([instance_masks[index]])
    cls = np.array([detection_classes[index]])
    score = np.array([detection_scores[index]])
    box = np.array([detection_boxes[index]])
    ymin, xmin, ymax, xmax = np.split(box, 4, axis=-1)
  processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)
  segmentations = coco_metric.generate_segmentation_from_masks(mask, processed_boxes, width, height)
  total_mask = segmentations[0]
  for mask in segmentations:
      total_mask = cv2.add(total_mask, mask)
  total_mask = total_mask.clip(0, 255).astype("uint8")
  output = cv2.bitwise_and(image,image,mask = total_mask)
  return output, True

